# Capstone Project: Gaussian Mixture Models (GMM) vs. Traditional Clustering  
## âœï¸ Authors
Binal Patel, Siddhi Naik, Kaleem Mohammed
## ğŸ“– Project Overview  
This project explores **Gaussian Mixture Models (GMM)** as an alternative to traditional clustering techniques, comparing it against **k-Means, Hierarchical Clustering, and DBSCAN** on three benchmark datasets. We analyze how GMMâ€™s **soft clustering** approach handles different data distributions, especially where traditional methods struggle.  

## ğŸ“Š Datasets Used  
We evaluate clustering performance on three synthetic datasets:  
1. **Moons Dataset** â€“ Non-spherical clusters (where GMM is expected to outperform k-Means).  
2. **Blobs Dataset** â€“ Well-separated clusters with varying densities.  
3. **Circles Dataset** â€“ Nested circular clusters (challenging for k-Means).  

## ğŸ” Key Objectives  
- **Understand GMMâ€™s working principles** (Expectation-Maximization, probability-based clustering).  
- **Implement GMM and compare it with k-Means, Hierarchical, and DBSCAN**.  
- **Analyze clustering performance using visualization and metrics (e.g., Silhouette Score, BIC/AIC for GMM).**  

## ğŸ—ï¸ Project Structure  
```
Machine_Learning_Capstone_Project/
â”‚â”€â”€ scripts/
â”‚ â”œâ”€â”€ main.ipynb # Jupyter Notebook with GMM implementation and comparisons
â”‚â”€â”€ plots/ # Clustering output visualizations
â”‚ â”œâ”€â”€ moons_clustering.png # GMM vs. traditional clustering on Moons Dataset
â”‚ â”œâ”€â”€ blobs_clustering.png # GMM vs. traditional clustering on Blobs Dataset
â”‚ â”œâ”€â”€ circles_clustering.png # GMM vs. traditional clustering on Circles Dataset
â”‚â”€â”€ README.md # Project description and instructions
â”‚â”€â”€ report.docx # Final written report
```


## ğŸ–¥ï¸ How to Run the Code
1. **Clone the repository**:
   ```bash
   git clone https://github.com/Kaleem-QADR/Machine_Learning_Capstone_Project.git
   cd Machine_Learning_Capstone_Project/scripts

   ```

2. Open Jupyter Notebook:
```
jupyter notebook main.ipynb
```
3. Run all cells to generate clustering outputs and comparisons.
4. Visualizations will be saved in the plots/ folder.

ğŸ“ˆ Performance Evaluation
Silhouette Score: Measures cluster quality for each method.
BIC & AIC (for GMM): Helps determine the optimal number of Gaussian components.
Clustering Plots: Visual comparison of GMM, k-Means, Hierarchical, and DBSCAN.


## ğŸ“Š Clustering Model Comparison Results  

### **1ï¸âƒ£ Silhouette Score Summary**  
| Dataset        | k-Means  | Hierarchical | DBSCAN  | GMM  |
|---------------|---------|-------------|--------|------|
| **Moons**     | **0.441** âœ… | 0.381 | 0.386 | 0.435 |
| **Blobs**     | **0.787** âœ… | 0.783 | 0.720 | 0.783 |
| **Circles**   | **0.381** âœ… | 0.351 | 0.004 âŒ | 0.381 |

### **2ï¸âƒ£ GMM Model Selection (BIC & AIC)**
| Dataset        | Optimal Components (BIC) | Optimal Components (AIC) |
|---------------|-------------------------|-------------------------|
| **Moons**     | 8 | 9 |
| **Blobs**     | **3** âœ… | 6 |
| **Circles**   | 9 | 9 |

### **ğŸ” Key Takeaways**  
âœ” **k-Means performed best for simple, well-separated clusters** (Blobs dataset).  
âœ” **GMM closely matched k-Means but allows probabilistic clustering**.  
âœ” **DBSCAN struggled with Circles Dataset, confirming density-based limitations**.  
âœ” **BIC & AIC helped determine the best number of Gaussian components for GMM**.  

---


ğŸ“Œ References
scikit-learn: GaussianMixture
Expectation-Maximization Algorithm
Clustering Evaluation Metrics

ğŸ“¢ Acknowledgments
Special thanks to our instructor and peers for their support and feedback on this project! ğŸ¯ğŸš€


---

### **What This README Includes**
âœ… **Project Overview**  
âœ… **Authors: Binal Patel, Siddhi Naik, Kaleem Mohammed**  
âœ… **Datasets & Objectives**  
âœ… **File Structure (Matches Your Preferred Format)**  
âœ… **How to Run the Code**  
âœ… **Evaluation Metrics** 
âœ… **Clustering Model Comparison Results**  
âœ… **References & Acknowledgments**  

---
